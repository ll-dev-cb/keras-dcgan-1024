{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"The code is implemented using Tensorflow and it is changed it to fully use Keras\"\n",
    "\"This will allown for saving weights now.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "import os\n",
    "print(os.listdir(\"inputs\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import glob\n",
    "from glob import glob\n",
    "import datetime\n",
    "import random\n",
    "from PIL import Image\n",
    "import matplotlib\n",
    "#matplotlib.use('agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# to choose which GPU should be used\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "#os.environ[\"HIP_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "\n",
    "# this section will allow code to be run on a RTX GPU\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)\n",
    "\n",
    "\n",
    "import warnings\n",
    "#warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1.keras as keras\n",
    "from tensorflow.compat.v1.keras.layers import Input, Dense, Reshape, Flatten, Dropout, GaussianNoise\n",
    "from tensorflow.compat.v1.keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from tensorflow.compat.v1.keras.layers import LeakyReLU\n",
    "from tensorflow.compat.v1.keras.layers import UpSampling2D, Conv2D , Conv2DTranspose\n",
    "from tensorflow.compat.v1.keras.models import Sequential, Model\n",
    "from tensorflow.compat.v1.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Generator\n",
    "def get_generator(z):\n",
    "    # 8 x 8 x 1024\n",
    "    input_layer = Input(z)\n",
    "    hid = Dense(8*8*1024, activation='relu', name =\"Dense\")(input_layer)    \n",
    "    hid = LeakyReLU(alpha=0.2)(hid)\n",
    "    hid = Reshape((8, 8, 1024))(hid)\n",
    "    hid = GaussianNoise(0.02)(hid)\n",
    "    \n",
    "    # 8x8x512 -> 16x16x256\n",
    "    hid = Conv2DTranspose(512, kernel_size=[5,5],\n",
    "                               strides=[2,2],\n",
    "                               padding=\"same\",\n",
    "                               kernel_initializer= keras.initializers.TruncatedNormal(stddev=WEIGHT_INIT_STDDEV),name =\"trans_conv1\")(hid)\n",
    "    hid = BatchNormalization(momentum=0.9, epsilon=EPSILON, name=\"batch_trans_conv1\")(hid)\n",
    "    hid = LeakyReLU(alpha=0.2,name =\"trans_conv_out\")(hid)\n",
    "    #hid = Activation(\"relu\", name =\"trans_conv_out\")(hid)\n",
    "    \n",
    "    # 8x8x512 -> 16x16x256\n",
    "    hid = Conv2DTranspose(512, kernel_size=[5,5],\n",
    "                               strides=[2,2],\n",
    "                               padding=\"same\",\n",
    "                               kernel_initializer= keras.initializers.TruncatedNormal(stddev=WEIGHT_INIT_STDDEV),name =\"trans_conv2\")(hid)\n",
    "    hid = BatchNormalization(momentum=0.9, epsilon=EPSILON, name=\"batch_trans_conv2\")(hid)\n",
    "    hid = LeakyReLU(alpha=0.2,name =\"trans_conv2_out\")(hid)\n",
    "    #hid = Activation(\"relu\", name =\"trans_conv2_out\")(hid)\n",
    "    \n",
    "    \n",
    "    # 16x16x256 -> 32x32x128\n",
    "    hid = Conv2DTranspose(256, kernel_size=[5,5],\n",
    "                               strides=[2,2],\n",
    "                               padding=\"same\",\n",
    "                               kernel_initializer= keras.initializers.TruncatedNormal(stddev=WEIGHT_INIT_STDDEV),name =\"trans_conv3\")(hid)\n",
    "    hid = BatchNormalization(momentum=0.9, epsilon=EPSILON, name=\"batch_trans_conv3\")(hid)\n",
    "    hid = LeakyReLU(alpha=0.2,name =\"trans_conv3_out\")(hid)\n",
    "    #hid = Activation(\"relu\", name =\"trans_conv3_out\")(hid)\n",
    "    \n",
    "\n",
    "    # 32x32x128 -> 64x64x64\n",
    "    hid = Conv2DTranspose(128, kernel_size=[5,5],\n",
    "                               strides=[2,2],\n",
    "                               padding=\"same\",\n",
    "                               kernel_initializer= keras.initializers.TruncatedNormal(stddev=WEIGHT_INIT_STDDEV),name =\"trans_conv4\")(hid)\n",
    "    hid = BatchNormalization(momentum=0.9, epsilon=EPSILON, name=\"batch_trans_conv4\")(hid)\n",
    "    hid = LeakyReLU(alpha=0.2,name =\"trans_conv4_out\")(hid)\n",
    "    #hid = Activation(\"relu\", name =\"trans_conv4_out\")(hid)  \n",
    "    \n",
    "    # 32x32x128 -> 64x64x64\n",
    "    hid = Conv2DTranspose(64, kernel_size=[5,5],\n",
    "                               strides=[2,2],\n",
    "                               padding=\"same\",\n",
    "                               kernel_initializer= keras.initializers.TruncatedNormal(stddev=WEIGHT_INIT_STDDEV),name =\"trans_conv5\")(hid)\n",
    "    hid = BatchNormalization(momentum=0.9, epsilon=EPSILON, name=\"batch_trans_conv5\")(hid)\n",
    "    hid = LeakyReLU(alpha=0.2,name =\"trans_conv5_out\")(hid)\n",
    "    #hid = Activation(\"relu\", name =\"trans_conv5_out\")(hid)\n",
    "    \n",
    "    # 32x32x128 -> 64x64x64\n",
    "    hid = Conv2DTranspose(64, kernel_size=[5,5],\n",
    "                               strides=[2,2],\n",
    "                               padding=\"same\",\n",
    "                               kernel_initializer= keras.initializers.TruncatedNormal(stddev=WEIGHT_INIT_STDDEV),name =\"trans_conv6\")(hid)\n",
    "    hid = BatchNormalization(momentum=0.9, epsilon=EPSILON, name=\"batch_trans_conv6\")(hid)\n",
    "    hid = LeakyReLU(alpha=0.2,name =\"trans_conv6_out\")(hid)\n",
    "    #hid = Activation(\"relu\", name =\"trans_conv6_out\")(hid)\n",
    "    \n",
    "    # 64x64x64 -> 64x64x3\n",
    "    hid = Conv2DTranspose(3, kernel_size=[5,5],\n",
    "                               strides=[2,2],\n",
    "                               padding=\"same\",\n",
    "                               kernel_initializer= keras.initializers.TruncatedNormal(stddev=WEIGHT_INIT_STDDEV),name =\"logits\")(hid)\n",
    "  \n",
    "    out = Activation(\"tanh\", name =\"out\")(hid)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=out)\n",
    "    print(\"Generator:\")\n",
    "    model.summary()\n",
    "  \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Discriminator\n",
    "\n",
    "def get_discriminator(input_shape):\n",
    "    # 64x64x3 -> 32x32x32\n",
    "    input_layer = Input(input_shape)\n",
    "    hid = Conv2D(filters=32,\n",
    "                    kernel_size=[5,5],\n",
    "                    strides=[2,2],\n",
    "                    padding=\"same\",\n",
    "                    kernel_initializer= keras.initializers.TruncatedNormal(stddev=WEIGHT_INIT_STDDEV),\n",
    "                    name = \"conv1\")(input_layer)\n",
    "    hid = BatchNormalization(momentum=0.9, epsilon=EPSILON, name=\"batch_norm1\")(hid)\n",
    "    hid = LeakyReLU(alpha=0.2, name=\"conv1_out\")(hid)\n",
    "    hid = Dropout(0.2)(hid)\n",
    "    \n",
    "    # 32x32x32-> 16x16x64 \n",
    "    hid = Conv2D(filters=64,\n",
    "                        kernel_size=[5,5],\n",
    "                        strides=[2,2],\n",
    "                        padding=\"same\",\n",
    "                        kernel_initializer= keras.initializers.TruncatedNormal(stddev=WEIGHT_INIT_STDDEV),\n",
    "                        name = \"conv2\")(hid)\n",
    "    hid = BatchNormalization(momentum=0.9, epsilon=EPSILON, name=\"batch_norm2\")(hid)\n",
    "    hid = LeakyReLU(alpha=0.2, name=\"conv2_out\")(hid) \n",
    "    hid = Dropout(0.2)(hid)\n",
    "    \n",
    "    # 16x16x64  -> 8x8x128  \n",
    "    hid = Conv2D(filters=128,\n",
    "                    kernel_size=[5,5],\n",
    "                    strides=[2,2],\n",
    "                    padding=\"same\",\n",
    "                    kernel_initializer= keras.initializers.TruncatedNormal(stddev=WEIGHT_INIT_STDDEV),\n",
    "                    name = \"conv3\")(hid)\n",
    "    hid = BatchNormalization(momentum=0.9, epsilon=EPSILON, name=\"batch_norm3\")(hid)\n",
    "    hid = LeakyReLU(alpha=0.2, name=\"conv3_out\")(hid)\n",
    "    hid = Dropout(0.2)(hid)\n",
    "    \n",
    "    # 8x8x128 -> 8x8x256\n",
    "    hid = Conv2D(filters=256,\n",
    "                    kernel_size=[5,5],\n",
    "                    strides=[2,2],\n",
    "                    padding=\"same\",\n",
    "                    kernel_initializer= keras.initializers.TruncatedNormal(stddev=WEIGHT_INIT_STDDEV),\n",
    "                    name = \"conv4\")(hid)\n",
    "    hid = BatchNormalization(momentum=0.9, epsilon=EPSILON, name=\"batch_norm4\")(hid)\n",
    "    hid = LeakyReLU(alpha=0.2, name=\"conv4_out\")(hid)\n",
    "    hid = Dropout(0.2)(hid)\n",
    "    \n",
    "    # 8x8x128 -> 8x8x256\n",
    "    hid = Conv2D(filters=256,\n",
    "                    kernel_size=[5,5],\n",
    "                    strides=[2,2],\n",
    "                    padding=\"same\",\n",
    "                    kernel_initializer= keras.initializers.TruncatedNormal(stddev=WEIGHT_INIT_STDDEV),\n",
    "                    name = \"conv5\")(hid)\n",
    "    hid = BatchNormalization(momentum=0.9, epsilon=EPSILON, name=\"batch_norm5\")(hid)\n",
    "    hid = LeakyReLU(alpha=0.2, name=\"conv5_out\")(hid)\n",
    "    \n",
    "\n",
    "    # 8x8x256 -> 4x4x512\n",
    "    hid = Conv2D(filters=512,\n",
    "                    kernel_size=[5,5],\n",
    "                    strides=[2,2],\n",
    "                    padding=\"same\",\n",
    "                    kernel_initializer= keras.initializers.TruncatedNormal(stddev=WEIGHT_INIT_STDDEV),\n",
    "                    name = \"conv6\")(hid)  \n",
    "    hid = BatchNormalization(momentum=0.9, epsilon=EPSILON, name=\"batch_norm6\")(hid)\n",
    "    hid = LeakyReLU(alpha=0.2, name=\"conv6_out\")(hid)\n",
    "    \n",
    "    # 8x8x256 -> 4x4x512\n",
    "    hid = Conv2D(filters=1024,\n",
    "                    kernel_size=[5,5],\n",
    "                    strides=[2,2],\n",
    "                    padding=\"same\",\n",
    "                    kernel_initializer= keras.initializers.TruncatedNormal(stddev=WEIGHT_INIT_STDDEV),\n",
    "                    name = \"conv7\")(hid)  \n",
    "    hid = BatchNormalization(momentum=0.9, epsilon=EPSILON, name=\"batch_norm7\")(hid)\n",
    "    hid = LeakyReLU(alpha=0.2, name=\"conv7_out\")(hid)\n",
    "    \n",
    "    \n",
    "    hid = Flatten(name = \"flatten\")(hid)\n",
    "\n",
    "    out = Dense(1, activation='sigmoid', name = \"ligit\")(hid)\n",
    "    model = Model(inputs= input_layer, outputs=out)\n",
    "    print(\"Discriminator:\")\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "IMAGE_SIZE = 1024\n",
    "NOISE_SIZE = 100\n",
    "\n",
    "LR_D = 0.00003\n",
    "LR_G = 0.0003\n",
    "BATCH_SIZE = 6\n",
    "EPOCHS = 2000 # For better results increase this value\n",
    "BETA1 = 0.9\n",
    "WEIGHT_INIT_STDDEV = 0.02\n",
    "EPSILON = 0.0005\n",
    "\n",
    "#runtime \n",
    "SAMPLES_TO_SHOW = 6\n",
    "SAVE_WEIGHTS_INTERVAL = 2\n",
    "SHOW_PROGRES_INTERVAL = 1\n",
    "LOAD_WEIGHTS_EP = 134"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "INPUT_DIR = \"rect\"\n",
    "INPUT_DATA_DIR = \"inputs/\" + INPUT_DIR + \"/\" # Path to the folder with input images. For more info check simspons_dataset.txt\n",
    "\n",
    "WEIGHTS_PATH = \"weights\" # for saving weights   \n",
    "if not os.path.exists(WEIGHTS_PATH):\n",
    "    os.makedirs(WEIGHTS_PATH)\n",
    "    \n",
    "OUTPUT_DIR = './gan-' + INPUT_DIR + '-{date:%Y-%m-%d_%H_%M_%S}/'.format(date=datetime.datetime.now())\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "    \n",
    "MODEL_NAME = OUTPUT_DIR + \"gan-\" + str(IMAGE_SIZE) + \"-\" + str(NOISE_SIZE) + \"-model.\" + str(LOAD_WEIGHTS_EP) + \".h5\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Discriminator\n",
    "discriminator = get_discriminator((IMAGE_SIZE, IMAGE_SIZE,3))\n",
    "\n",
    "\n",
    "#discriminator.trainable = False\n",
    "\n",
    "discriminator.compile(loss='binary_crossentropy',optimizer=Adam(lr=LR_D, beta_1=BETA1),metrics=['accuracy'])\n",
    "#discriminator.load_weights(\"weights/disc-512-weights.\" + str(LOAD_WEIGHTS_EP) + \".h5\");\n",
    "# For the combined model we will only train the generator\n",
    "discriminator.trainable = False\n",
    "\n",
    "#Generator\n",
    "generator = get_generator((NOISE_SIZE,))\n",
    "\n",
    "#GAN\n",
    "# The discriminator takes generated images as input and determines validity\n",
    "gan_input = Input(shape=(NOISE_SIZE,))\n",
    "x = generator(gan_input)\n",
    "gan_out = discriminator(x)\n",
    "gan = Model(gan_input, gan_out)\n",
    "gan.summary()\n",
    "\n",
    "gan.compile(loss='binary_crossentropy',optimizer=Adam(lr=LR_G, beta_1=BETA1))\n",
    "if (LOAD_WEIGHTS_EP != 0):\n",
    "    gan.load_weights(\"weights/gan-1024-\" + str(NOISE_SIZE) + \"-weights.\" + str(LOAD_WEIGHTS_EP) + \".h5\", by_name=True)\n",
    "\n",
    "#gan.save(\"models/gan-1024-model.\" + str(LOAD_WEIGHTS_EP) + \".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_samples(sample_images, name, epoch):\n",
    "    figure, axes = plt.subplots(1, len(sample_images), figsize = (IMAGE_SIZE/8, IMAGE_SIZE/8))\n",
    "    for index, axis in enumerate(axes):\n",
    "        axis.axis('off')\n",
    "        image_array = sample_images[index]\n",
    "        axis.imshow(image_array)\n",
    "        image = Image.fromarray(image_array)\n",
    "        image.save(name+\"_\"+str(epoch)+\"_\"+str(index)+\".png\") \n",
    "    plt.savefig(name+\"_\"+str(epoch)+\".png\", bbox_inches='tight', pad_inches=0)\n",
    "    print(\"save images\")\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(input_z, epoch):\n",
    "    samples = generator.predict(input_z[:SAMPLES_TO_SHOW])\n",
    "    sample_images = [((sample + 1.0) * 127.5).astype(np.uint8) for sample in samples]\n",
    "    show_samples(sample_images, OUTPUT_DIR + \"samples\", epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_epoch(d_losses, g_losses , data_shape, epoch, duration, input_z):\n",
    "    minibatch_size = int(data_shape[0]//BATCH_SIZE)\n",
    "    print(\"Epoch {}/{}\".format(epoch, EPOCHS),\n",
    "          \"\\nDuration: {:.5f}\".format(duration),\n",
    "          \"\\nD Loss: {:.5f}\".format(np.mean(d_losses[-minibatch_size:])),\n",
    "          \"\\nG Loss: {:.5f}\".format(np.mean(g_losses[-minibatch_size:])))\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.plot(d_losses, label='Discriminator', alpha=0.6)\n",
    "    plt.plot(g_losses, label='Generator', alpha=0.6)\n",
    "    plt.title(\"Losses\")\n",
    "    plt.legend()\n",
    "    plt.savefig(OUTPUT_DIR + \"losses_\" + str(epoch) + \".png\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    test(input_z, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(data):\n",
    "    batches = []\n",
    "    for i in range(int(data.shape[0]//BATCH_SIZE)):\n",
    "        #debug - \n",
    "        #print(\"get_batches_1: \" + str(i))\n",
    "        batch = data[i * BATCH_SIZE:(i + 1) * BATCH_SIZE]\n",
    "        augmented_images = []\n",
    "        for img in batch:\n",
    "            image = Image.fromarray(img)\n",
    "            if random.choice([True, False]):\n",
    "                image = image.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "            augmented_images.append(np.asarray(image, dtype=np.float32))\n",
    "        batch = np.asarray(augmented_images, dtype=np.float32)\n",
    "        normalized_batch = (batch / 127.5) - 1.0        \n",
    "        batches.append(normalized_batch)\n",
    "    return np.array(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Load Images\n",
    "\n",
    "print(\"Load Sample Images\")\n",
    "input_images = np.asarray([np.asarray(Image.open(file).resize((IMAGE_SIZE, IMAGE_SIZE))) for file in glob(INPUT_DATA_DIR + '*')])\n",
    "\n",
    "print (\"Input: \" + str(input_images.shape))\n",
    "\n",
    "np.random.shuffle(input_images)\n",
    "\n",
    "print(\"done shuffeling\")\n",
    "\n",
    "sample_images = random.sample(list(input_images), SAMPLES_TO_SHOW)\n",
    "print(\"get random samples\")\n",
    "\n",
    "show_samples(sample_images, OUTPUT_DIR + \"inputs\", 0)\n",
    "print(\"show samples\")\n",
    "\n",
    "# Training\n",
    "print(\"Training Starts!\")\n",
    "\n",
    "#warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "d_losses = []\n",
    "g_losses = []\n",
    "cum_d_loss = 0\n",
    "cum_g_loss = 0\n",
    "\n",
    "#EPOCHS = 50\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch += 1\n",
    "    start_time = time.time()\n",
    "    batch =0;\n",
    "    \n",
    "    for batch_images in get_batches(input_images):\n",
    "        batch +=1;\n",
    "        #print (\"training batch \" + str(batch))\n",
    "        noise_data = np.random.normal(0, 1, size=(BATCH_SIZE, NOISE_SIZE))\n",
    "        # We use same labels for generated images as in the real training batch\n",
    "        generated_images = generator.predict(noise_data)\n",
    "        \n",
    "        noise_prop = 0.05 # Randomly flip 5% of targets\n",
    "        real_labels = np.zeros((BATCH_SIZE, 1)) + np.random.uniform(low=0.0, high=0.1, size=(BATCH_SIZE, 1))\n",
    "        flipped_idx = np.random.choice(np.arange(len(real_labels)), size=int(noise_prop*len(real_labels)))\n",
    "        real_labels[flipped_idx] = 1 - real_labels[flipped_idx]\n",
    "        \n",
    "        # Train discriminator on real data\n",
    "        d_loss_real = discriminator.train_on_batch(batch_images, real_labels)\n",
    "\n",
    "\n",
    "        # Prepare labels for generated data\n",
    "        fake_labels = np.ones((BATCH_SIZE, 1)) - np.random.uniform(low=0.0, high=0.1, size=(BATCH_SIZE, 1))\n",
    "        flipped_idx = np.random.choice(np.arange(len(fake_labels)), size=int(noise_prop*len(fake_labels)))\n",
    "        fake_labels[flipped_idx] = 1 - fake_labels[flipped_idx]\n",
    "        \n",
    "        # Train discriminator on generated data\n",
    "        d_loss_fake = discriminator.train_on_batch(generated_images, fake_labels)\n",
    "        \n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "        cum_d_loss += d_loss\n",
    "        d_losses.append(d_loss[0])\n",
    "        \n",
    "        \n",
    "        # Train generator\n",
    "        noise_data = np.random.normal(0, 1, size=(BATCH_SIZE, NOISE_SIZE))\n",
    "        g_loss = gan.train_on_batch(noise_data, np.zeros((BATCH_SIZE, 1)))\n",
    "        cum_g_loss += g_loss\n",
    "        g_losses.append(g_loss)\n",
    "        \n",
    "    if epoch > 0 and epoch % SAVE_WEIGHTS_INTERVAL == 0 :\n",
    "        print(\"saving model\")\n",
    "        aepochs = epoch + LOAD_WEIGHTS_EP\n",
    "        #discriminator.save_weights(\"weights/disc-1024-weights.\"  + str(aepochs) + \".h5\")\n",
    "        #generator.save_weights(\"weights/gen-1024-weights.\"  + str(aepochs) + \".h5\")\n",
    "        gan.save_weights(OUTPUT_DIR + \"gan-1024-\" + str(NOISE_SIZE) + \"-weights.\" + str(aepochs) + \".h5\")\n",
    "        print(\"saving\")\n",
    "    \n",
    "    print(\"end epoch: \" + str(epoch))    \n",
    "    print(\"time elapsed: {:.5f}\".format(time.time()-start_time))\n",
    "  \n",
    "    # Plot the progress\n",
    "    if epoch > 0 and epoch % SHOW_PROGRES_INTERVAL == 0 :\n",
    "        summarize_epoch(d_losses, g_losses, input_images.shape, epoch, time.time()-start_time, noise_data)\n",
    "    batch_images = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
